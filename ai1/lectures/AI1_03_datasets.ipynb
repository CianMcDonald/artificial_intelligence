{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Datasets</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.random import rand\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Features</h1>\n",
    "<ul>\n",
    "    <li>Suppose we want to store data about objects, such as houses.</li>\n",
    "    <li><b>Features</b> describe the houses, e.g.\n",
    "        <ul>\n",
    "            <li>$\\mathit{flarea}$: the total floor area (in square metres);</li>\n",
    "            <li>$\\mathit{bdrms}$: the number of bedrooms;</li>\n",
    "            <li> $\\mathit{bthrms}$: the number of bathrooms.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>A particular house has <b>values</b> for the features:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\mathit{flarea} = 126, \\mathit{bdrms} = 3, \\mathit{bthrms} = 1$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we can represent a house using a vector:\n",
    "        <ul>\n",
    "            <li>e.g. your house: $\\cv{126\\\\3\\\\1}$\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will always use $n$ to refer to the number of features, e.g. above $n = 3$.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Examples</h1> \n",
    "<ul>\n",
    "    <li>Suppose we collect a <b>dataset</b> containing data about lots of houses, e.g.:\n",
    "        $$\\cv{126\\\\3\\\\1} \\,\\, \\cv{92.9\\\\3\\\\2} \\,\\,\\cv{171.9\\\\4\\\\3} \\,\\, \\cv{79\\\\3\\\\1}$$\n",
    "    </li>\n",
    "    <li>Each member of this dataset is called an <b>example</b>, and we will use $m$ to refer to the number of examples, e.g.\n",
    "        above $m = 4$.\n",
    "    </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset notation</h1>\n",
    "<ul>\n",
    "    <li>We will use a <em>superscript</em> to index the examples.\n",
    "        <ul>\n",
    "            <li>\n",
    "                $\\v{x}^{(i)}$ will be the $i$th example.\n",
    "            </li>\n",
    "            <li>\n",
    "                The first example in the dataset is $\\v{x}^{(1)}$, the second is $\\v{x}^{(2)}$, $\\ldots$, \n",
    "                the last is $\\v{x}^{(m)}$ (Note, we index from 1.)\n",
    "            </li>\n",
    "            <li>\n",
    "                We're writing the superscript in parentheses to make it clear that we are using it for indexing.\n",
    "                It is not 'raising to a power'. If we want to raise to a power, we will drop the parentheses.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We will use a <em>subscript</em> to index the features (again starting from 1).</li>\n",
    "    <li>Class exercise. Using the dataset from above:\n",
    "        <ul>\n",
    "            <li>what is $\\v{x}_2^{(1)}$?</li>\n",
    "            <li>what is $\\v{x}_1^{(2)}$?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset as a matrix</h1>\n",
    "<ul>\n",
    "    <li>We can represent a dataset $\\Set{\\v{x}^{(1)}, \\v{x}^{(2)}, \\ldots, \\v{x}^{(m)}}$ as a $m \\times n$\n",
    "        matrix $\\v{X}$ as follows:\n",
    "        $$\\v{X} = \\begin{bmatrix}\n",
    "              \\v{x}_1^{(1)} & \\v{x}_2^{(1)} & \\ldots & \\v{x}_n^{(1)} \\\\\n",
    "              \\v{x}_1^{(2)} & \\v{x}_2^{(2)} & \\ldots & \\v{x}_n^{(2)} \\\\\n",
    "              \\vdots        & \\vdots        & \\vdots & \\vdots \\\\\n",
    "              \\v{x}_1^{(m)} & \\v{x}_2^{(m)} & \\ldots & \\v{x}_n^{(m)} \\\\\n",
    "              \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "    <li>Note how each example becomes a <em>row</em> in $\\v{X}$.</li>\n",
    "    <li>You can think of row $i$ as the transpose of $\\v{x}^{(i)}$.</li>\n",
    "    <li>For the example dataset, we get:\n",
    "        $$\\v{X} = \n",
    "            \\begin{bmatrix}\n",
    "                126 & 3 & 1 \\\\\n",
    "                92.9 & 3 & 2 \\\\\n",
    "                171.9 & 4 & 3 \\\\\n",
    "                79 & 3 & 1\n",
    "            \\end{bmatrix}\n",
    "        $$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Cork Property Prices Dataset</h1>\n",
    "<ul>\n",
    "    <li>In August 2019, I scraped a dataset of property prices for Cork city from www.daft.ie.</li>\n",
    "    <li>They are in a CSV file. Each line in the file is an example, representing one house.</li>\n",
    "    <li>Hence, each line of the file contains the feature-values for the floor area, number of bedrooms, number of\n",
    "        bathrooms, and several other features that we will ignore for now.\n",
    "    </li>\n",
    "    <li>We will use the pandas library:\n",
    "        <ul>\n",
    "            <li>to read the dataset from the csv file into what pandas calls a DataFrame;</li>\n",
    "            <li>to explore the dataset: looking at values and computing summary statistics.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Then we will extract some of the features (columns) and convert to a numpy 2D array, before using the data\n",
    "        to find houses similar to yours.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Using pandas to Read and Explore the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 4)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flarea', 'bdrms', 'bthrms', 'price'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    float64\n",
       "bdrms       int64\n",
       "bthrms      int64\n",
       "price       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>125.460151</td>\n",
       "      <td>3.329741</td>\n",
       "      <td>2.120690</td>\n",
       "      <td>352.297414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>70.692202</td>\n",
       "      <td>1.068445</td>\n",
       "      <td>1.061033</td>\n",
       "      <td>197.464495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>295.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.600000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>575.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1495.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flarea       bdrms      bthrms        price\n",
       "count  464.000000  464.000000  464.000000   464.000000\n",
       "mean   125.460151    3.329741    2.120690   352.297414\n",
       "std     70.692202    1.068445    1.061033   197.464495\n",
       "min     40.000000    1.000000    1.000000    95.000000\n",
       "25%     82.000000    3.000000    1.000000   235.000000\n",
       "50%    110.000000    3.000000    2.000000   295.000000\n",
       "75%    140.600000    4.000000    3.000000   395.000000\n",
       "max    575.000000    9.000000    6.000000  1495.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111.9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flarea  bdrms  bthrms  price\n",
       "0   111.9      3       3    305\n",
       "1    95.0      3       3    255\n",
       "2   120.8      3       3    275"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A few of the examples\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Convert to a numpy 2D array</h2>\n",
    "<ul>\n",
    "    <li>We will select certain features (columns) from the pandas DataFrame\n",
    "        and convert to a 2D numpy array\n",
    "    </li>\n",
    "    <li>(Later in the module, we will use a <code>ColumnTransformer</code> to do this.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract these features and convert to numy 2D array\n",
    "X = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111.9,   3. ,   3. ],\n",
       "       [ 95. ,   3. ,   3. ],\n",
       "       [120.8,   3. ,   3. ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X - to show you that we now have a 2D numpy array\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Similarity &amp; Distance</h1>\n",
    "<ul>\n",
    "    <li>In AI, we often want to know how <em>similar</em> one object is to another.\n",
    "        <ul>\n",
    "            <li>E.g. how similar is my house to yours?</li>\n",
    "            <li>E.g. which house in our dataset is most similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In fact, here we are instead going to measure how <em>different</em> they are using a <b>distance function</b>.\n",
    "        <ul>\n",
    "            <li>(N.B. This is not about geographical distance.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Let $\\v{x}$ be one vector of feature values and $\\v{x}'$ be another.</li>\n",
    "    <li>Simplest is to measure their <b>Euclidean distance</b>:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{(\\v{x}_1 - \\v{x}_1')^2 + (\\v{x}_2 - \\v{x}_2')^2 + \\ldots + (\\v{x}_n - \\v{x}_n')^2}$$\n",
    "        or, more concisely:\n",
    "        $$d(\\v{x}, \\v{x}') = \\sqrt{\\sum_{j=1}^n(\\v{x}_j - \\v{x}_j')^2}$$\n",
    "    </li>\n",
    "    <li>Euclidean distance has a minimum value of 0 (meaning identical) but no maximum value (depends on your data).</li>\n",
    "    <li>Class exercise. What is the Euclidean distance between $\\v{x} = \\cv{100\\\\1\\\\4}$ and $\\v{x}' = \\cv{100\\\\5\\\\1}$?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Euclidean Distance in numpy</h2>\n",
    "<ul>\n",
    "    <li>It has a nice vectorized implementation (no loop!) using numpy:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc(x, xprime):\n",
    "    return np.sqrt(np.sum((x - xprime)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.026297590440446"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "your_house = np.array([126.0, 3, 1])\n",
    "my_house = np.array([107.0, 2, 1])\n",
    "\n",
    "euc(your_house, my_house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute the distance between your house and all the houses in X.</li>\n",
    "    <li>(We have to write a loop here, because our <code>euc</code> function is not vectorized.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [euc(your_house, x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.241137595009741, 31.064449134018133, 5.571355310873651]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show you, here are the first 3 distances\n",
    "dists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0332473082471605"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better, we can, with one line of code, find the most similar house\n",
    "np.min([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even better again, we can find which house is the most similar\n",
    "np.argmin([euc(your_house, x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    125.74\n",
       "bdrms       3.00\n",
       "bthrms      2.00\n",
       "price     398.00\n",
       "Name: 196, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best of all, we can display the most similar house\n",
    "df.iloc[np.argmin([euc(your_house, x) for x in X])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Euclidean distance</h1>\n",
    "<ul>\n",
    "    <li>There are at least two problems with Euclidean distance (and many other distance measures too):\n",
    "        <ul>\n",
    "            <li>Features with different scales;</li>\n",
    "            <li>Features that are correlated with each other;</li>\n",
    "            <li>The curse of dimensionality.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Scaling Numeric Values</h1>\n",
    "<ul>\n",
    "    <li>Different numeric-valued features often have very different ranges.\n",
    "        <ul>\n",
    "            <li>E.g. the values for floor area are going to range from a few tens to a few hundreds of square metres.</li>\n",
    "            <li>But the number of bedrooms and bathrooms is going to range from 0 to a dozen or so at most.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When computing the Euclidean distance, features with large ranges will dominate the distance calculations, \n",
    "        thus giving features with small ranges negligible influence.\n",
    "    </li>\n",
    "    <li>\n",
    "        E.g., consider your house $\\v{x} = \\cv{126\\\\3\\\\1}$ and two others, $\\v{y} = \\cv{131\\\\3\\\\1}$ and\n",
    "        $\\v{z} = \\cv{126\\\\7\\\\1}$. \n",
    "        <ul>\n",
    "            <li><em>Intuitively</em>, which house is more similar to yours, $\\v{y}$ or $\\v{z}$?</li>\n",
    "            <li>Now compute the Euclidean distances.</li>\n",
    "            <li>According to these distances, which house is more similar to yours?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        The solution is to <b>scale</b> (or 'normalize') the values so that they have similar ranges.\n",
    "    </li>\n",
    "    <li>There are several ways to do this. One is <b>min-max scaling</b>, but the one we'll discuss is <b>standardization</b>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization</h2>\n",
    "<ul>\n",
    "    <!--\n",
    "    <li>In some cases, you don't want feature values to have the same range but to have the same mean\n",
    "        and even the same variance\n",
    "    </li>\n",
    "    -->\n",
    "    <li>\n",
    "        One idea is <b>mean centering</b>, where you subtract the mean value of the feature.\n",
    "        <ul>\n",
    "            <li>If you do this to all values, some of the new values will be positive and some will be negative and \n",
    "                their mean will be approximately zero.\n",
    "                </li>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But better still is <b>standardization</b>, in which you subtract the mean and divide by the standard\n",
    "        deviation:\n",
    "        $$\\v{x}_j \\gets \\frac{\\v{x}_j - \\mu_j}{\\sigma_j}$$\n",
    "        where $\\mu_j$ is the mean of the values for feature $j$ and $\\sigma_j$ is their standard deviation\n",
    "    </li>\n",
    "    <li>\n",
    "        If you use this, then the mean will be approximately zero, the standard deviation will be 1.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Standardization in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>scikit-learn provides a class called <code>StandardScaler</code>.\n",
    "    </li>\n",
    "    <li>It uses means and standard deviations that it calculates from your dataset. (Statisticians would say that it should\n",
    "        use the population mean and standard deviation, but these are generally not known.)\n",
    "    </li>\n",
    "    <li>We create the scaler and then run its <code>fit</code> and <code>transform</code> methods.</li>\n",
    "    <li>(Later in the module, when we are using a <code>ColumnTransformer</code>, running these methods\n",
    "        will be done for us.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19202665, -0.30895098,  0.82962481],\n",
       "       [-0.43134924, -0.30895098,  0.82962481],\n",
       "       [-0.06599286, -0.30895098,  0.82962481]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at a few rows in X\n",
    "X_scaled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00764486, -0.30895098, -1.05736495])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's scale your house too\n",
    "# Don't try to understand or copy this code - it's a hack that you won't need\n",
    "your_house = np.array([[126.0, 3, 1]])\n",
    "your_house_scaled = scaler.transform(your_house)[0]\n",
    "your_house_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see what effect this has had, let's see which house is most similar to yours\n",
    "np.argmin([euc(your_house_scaled, x) for x in X_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flarea    122.4\n",
       "bdrms       3.0\n",
       "bthrms      1.0\n",
       "price     295.0\n",
       "Name: 328, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argmin([euc(your_house_scaled, x) for x in X_scaled])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Features that are Correlated</h1>\n",
    "<ul>\n",
    "    <li>Let's start with an extreme example. \n",
    "        <ul>\n",
    "            <li>Suppose one feature is the floor area in square metres and\n",
    "                another is the floor area in square feet.\n",
    "                Then it's clear that, even after scaling, when calculating distances, floor area will have greater\n",
    "                influence than other features, such as the number of bedrooms, because it is in the dataset twice.\n",
    "            </li>\n",
    "            <li>Examples are often less stark. For example, floor area and the number of bedrooms are correlated,\n",
    "                and so their contributions to the distance calculations are not independent of each other.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Ideally the features should be independent (at least, linearly independent).</li>\n",
    "    <li>Yet, few people who use distances do anything about this problem!</li>\n",
    "    <li>Solution (which we're not covering in detail) include feature weighting and projections to\n",
    "        a new feature space whose features are (linearly) independent (e.g. using Principal Component\n",
    "        Analysis).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Curse of Dimensionality</h1>\n",
    "<ul>\n",
    "    <li>In some datasets, examples have thousands or even millions of features.\n",
    "        <ul>\n",
    "            <li>E.g. datasets from astronomy;</li>\n",
    "            <li>E.g. datasets of images and videos;</li>\n",
    "            <li>E.g. datasets of documents where each unique word is a feature.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is it better or worse to have more features?\n",
    "        <ul>\n",
    "            <li>Storage and processing costs increase.</li>\n",
    "            <li>Apart from efficiency, intuitively, more features is better:\n",
    "                <ul>\n",
    "                    <li>e.g. describing houses more completely.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>But, counter-intuitively, that isn't true in general.\n",
    "                <ul>\n",
    "                    <li>As the number of features grows, algorithms that use distance and density, will find it harder \n",
    "                        to find good solutions.\n",
    "                    </li>\n",
    "                    <li>The problems that arise as the number of features grows have been called <b>the curse of dimensionality</b>.\n",
    "    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of the Curse of Dimensionality</h2>\n",
    "<ul>\n",
    "    <li>The code that follows (which you don't need to study):\n",
    "        <ul>\n",
    "            <li>generates a random dataset where $m = 400$ and $n = 2$ and both features have values in $[0, 1)$;\n",
    "            </li>\n",
    "            <li>computes the Euclidean distance between all pairs of examples;</li>\n",
    "            <li>finds $d_{\\mathit{min}}$, the smallest of these distances;</li>\n",
    "            <li>finds $d_{\\mathit{max}}$, the largest of the distances;</li>\n",
    "            <li>computes the ratio $\\frac{d_{\\mathit{max}}}{d_{\\mathit{min}}}$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>It then does this all again but with $n = 3, 4, 5,\\ldots,500$.</li>\n",
    "    <li>Then it plots the ratios that it has computed ($y$-axis, but note its scale) against $n$ ($x$-axis).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 400\n",
    "n_range = range(1, 500)\n",
    "\n",
    "ratios = []\n",
    "for n in n_range:\n",
    "    X = rand(m, n)\n",
    "    dists = euclidean_distances(X)\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    ratios += [np.max(non_zero_dists) / (np.min(non_zero_dists))]\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Ratio of max distance to min distance in datasets with ever more features\")\n",
    "plt.scatter(n_range, ratios)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"n\")\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel(\"ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>As $n \\rightarrow \\infty$, $d_{\\mathit{max}} \\rightarrow d_{\\mathit{min}}$, so their ratio tends to 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2431123367385082,\n",
       " 1.2747844804800637,\n",
       " 1.2788368843139524,\n",
       " 1.2470548747783454,\n",
       " 1.2505332854968303]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it may not be clear from the graph, we'll show the last 5 of the ratios that it calculated\n",
    "ratios[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We conclude (counter-intutively) that examples become equi-distant!</li>\n",
    "    <li>This obviously undermines methods that depend on finding objects that are similar to each other, as we were\n",
    "        doing earlier &mdash; with more features, the most similar object becomes more arbitrary!\n",
    "    </li>\n",
    "    <li>The problem extends to other distance/similarity measures, e.g. cosine similarity.</li>\n",
    "    <li>Fortunately, there are lots of methods available for reducing dimensionality.\n",
    "        One solution is to retain the principle components found by Principal Component Analysis. This is\n",
    "        interesting because PCA was suggested above as a solution to the problem of correlated features. \n",
    "        It can actually help us solve both problems.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
