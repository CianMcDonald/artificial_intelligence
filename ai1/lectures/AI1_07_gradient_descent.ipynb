{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Gradient Descent</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "    <li>I based 5 of the diagrams on ones to be found in A. G&eacute;ron: <i>Hands-On Machine Learning with Scikit-Learn, Keras &amp;\n",
    "        TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li><b>Gradient Descent</b> is a generic method for finding optimal solutions to problems that involve\n",
    "        minimizing a loss function.\n",
    "    </li>\n",
    "    <li>It is a <em>search</em> in the model's <b>parameter space</b> for values of the parameters that minimize \n",
    "        the loss function.\n",
    "    </li>\n",
    "    <li>Conceptually:\n",
    "        <ul>\n",
    "            <li>\n",
    "                 It starts with an initial guess for the values of the parameters.\n",
    "            </li>\n",
    "            <li>\n",
    "                Then repeatedly:\n",
    "                <ul>\n",
    "                    <li>It updates the parameter values  &mdash; hopefully to reduce the loss.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        <img src=\"images/fog.jpg\" alt=\"\" />\n",
    "    </li>\n",
    "    <li>\n",
    "        Ideally, it keeps doing this until <b>convergence</b> &mdash; changes to the parameter values do not result\n",
    "        in lower loss.\n",
    "    </li>\n",
    "    <li>The key to this algorithm is how to update the parameter values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>The update rule</h2>\n",
    "<ul>\n",
    "     <li>To update the parameter values to reduce the loss:\n",
    "         <ul>\n",
    "             <li>Compute the gradient vector.\n",
    "                 <ul>\n",
    "                     <li>But this points 'uphill' and we want to go 'downhill'.</li>\n",
    "                     <li>And we want to make 'baby steps' (see later), so we use a <b>learning rate</b>, \n",
    "                         $\\alpha$, which is between 0 and 1.\n",
    "                     </li>\n",
    "                 </ul>\n",
    "             </li>\n",
    "             <li>So subtract $\\alpha$ times the gradient vector from $\\v{\\beta}$.</li>\n",
    "         </ul>\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\nabla_{\\v{\\beta}}J(\\v{X}, \\v{y}, \\v{\\beta})$$\n",
    "         Or\n",
    "         $$\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$$\n",
    "     </li>\n",
    "     <li>(BTW, this is vectorized. Naive loop implementations are wrong: they lose the\n",
    "         <em>simultaneous</em> update of the $\\v{\\beta}_j$.)\n",
    "     </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Gradient descent algorithm</h2>\n",
    "<ul>\n",
    "    <li>Pseudocode (in fact, this is for <b>batch gradient descent</b>, see later):\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baby steps</h2>\n",
    "<ul>\n",
    "    <li>We'll use  an example with a single feature/single parameter $\\beta_1$ in order to visualize.</li>\n",
    "    <li>We update $\\beta_1$ gradually, one baby step at a time, unitl the algorithm converges on minimum loss:\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps1.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>The size of the steps is determined by <!--a <b>hyperparameter</b> called--> the learning rate.\n",
    "    <!--\n",
    "        <ul>\n",
    "            <li>(Hyperparamters are explained in CS4619)</li>\n",
    "        </ul>\n",
    "       -->\n",
    "    </li>\n",
    "    <li>If the learning rate is too small, it will take many updates until convergence:\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps2.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>If the learning rate is too big, the algorithm might jump across the valley &mdash; it may even end up with\n",
    "        higher loss than before, making the next step bigger.\n",
    "        <ul>\n",
    "            <li>This might make the algorithm <b>diverge</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "        <figure>\n",
    "            <img src=\"images/baby_steps3.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why we need to scale for Gradient Descent</h2>\n",
    "<ul>\n",
    "    <li>If we are doing OLS regression using the Normal Equation, we do not need to scale the features.\n",
    "        But if we are doing OLS regression using Gradient Descent, we do need to scale the features.\n",
    "    </li>\n",
    "    <li>If features have different ranges, it affects the shape of the 'bowl'.</li>\n",
    "    <li>E.g. features 1 and 2 have similar ranges of values &mdash; a 'bowl':\n",
    "        <figure>\n",
    "            <img src=\"images/scaled.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>The algorithm goes straight towards the minimum.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>E.g. feature 1 has smaller values than feature 2 &mdash; an elongated 'bowl':\n",
    "        <figure>\n",
    "            <img src=\"images/unscaled.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>Since feature 1 has smaller values, it takes a larger change in $\\v{\\beta}_1$ to affect \n",
    "                the loss function, which is why it is elongated.\n",
    "            </li>\n",
    "            <li>It takes more steps to get to the minimum &mdash; steeply down but not really towards the\n",
    "                goal, followed by a long march down a nearly flat valley.\n",
    "            </li>\n",
    "            <li>It makes it more difficult to choose a value for the learning rate that avoids diveregence:\n",
    "                a value that suits one feature may not suit another.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Variants of Gradient Descent</h2>\n",
    "<ul>\n",
    "    <li>There are, in fact, three variants:\n",
    "        <ul>\n",
    "            <li>Batch Gradient Descent;</li>\n",
    "            <li>Stochastic Gradient Descent; and</li>\n",
    "            <li>Mini-batch Gradient Descent.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>The pseudocode we saw earlier (repeated here for convenience) is Batch Gradient Descent:\n",
    "        <ul style=\"background: lightgrey; list-style: none\">\n",
    "            <li>initialize $\\v{\\beta}$ randomly\n",
    "            <li>\n",
    "                repeat until convergence\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        $\\v{\\beta} \\gets \\v{\\beta} - \\frac{\\alpha}{m}\\v{X}^T(\\v{X}\\v{\\beta} - \\v{y})$\n",
    "                    </li>\n",
    "                </ul>\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Why is it called <em>Batch</em> Gradient Descent?\n",
    "        <ul>\n",
    "            <li>The update involves a calculation over the <em>entire</em> training set $\\v{X}$\n",
    "                on every iteration.\n",
    "            </li>\n",
    "            <li>This can be slow for large training sets.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Batch Gradient Descent in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement it ourselves.</li>\n",
    "    <li>Again for the purposes of this explanation, we will use the entire dataset as our training set.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for OLS regression (assumes X contains all 1s in its first column)\n",
    "def J(X, y, beta):\n",
    "    return np.mean((X.dot(beta) - y) ** 2) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent_for_ols_linear_regression(X, y, alpha, num_iterations):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_iterations)\n",
    "    \n",
    "    for iter in range(num_iterations):\n",
    "        beta -= (1.0 * alpha / m) * X.T.dot(X.dot(beta) - y)\n",
    "        Jvals[iter] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([352.29732796, 175.19885927,   0.3586059 ,   1.4669502 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Batch Gradient Descent\n",
    "beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_iterations = 500)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Bear in mind that the coefficients it finds are on the scaled data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>It's a good idea to plot the values of the loss function against the number of iterations.\n",
    "    </li>\n",
    "    <li>For OLS regression done using Batch Gradient Descent, if the loss ever increases, then:\n",
    "        <ul>\n",
    "            <li>\n",
    "                the code might be incorrect; or\n",
    "            </li>\n",
    "            <li>\n",
    "                the value of $\\alpha$ is too big and is causing divergence.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>The algorithm gives us the problem of choosing the number of iterations.</li>\n",
    "    <li>An alternative is to use a very large number of iterations but exit when the gradient vector\n",
    "        becomes tiny:\n",
    "        <ul>\n",
    "            <li>when its norm becomes smaller than <b>tolerance</b>, $\\eta$.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Here's an interactive version that allows you to choose the value of $\\alpha$ and to decide\n",
    "        whether to scale the data or not.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0a5f89d13442719ab3ab1812b1d3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='scale'), Dropdown(description='alpha', options=(('0.00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bgd(scale=True, alpha=0.03):\n",
    "    # Get the feature-values and the target values \n",
    "    X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "    y = df[\"price\"].values\n",
    "    # Scale the data, if requested\n",
    "    if scale:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "    # Add the extra column to X\n",
    "    X = add_dummy_feature(X)\n",
    "    # Run the Batch Gradient Descent\n",
    "    beta, Jvals = batch_gradient_descent_for_ols_linear_regression(X, y, alpha, num_iterations = 3000)\n",
    "    # Display beta\n",
    "    print(\"beta: \", beta)\n",
    "    # Plot loss\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.title(\"$J$ during learning\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.xlim(1, Jvals.size)\n",
    "    plt.ylabel(\"$J$\")\n",
    "    plt.ylim(3500, 50000)\n",
    "    xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "    plt.scatter(xvals, Jvals)\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot = interactive(bgd, {'manual': True}, \n",
    "    scale=True, alpha=[(\"0.00009\", 0.00009), (\"0.0009\", 0.0009), (\"0.009\", 0.009), (\"0.09\", 0.09), (\"0.9\", 0.9)]) \n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        Some people suggest a variant of Batch Gradient Descent in which the value of $\\alpha$ is decreased\n",
    "        over time, i.e. its value in later iterations is smaller\n",
    "        <ul>\n",
    "            <li>Why do they suggest this? </li>\n",
    "            <li>And why isn't it necessary?\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(But, we'll revisit this idea in Stochastic Gradient Descent.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Stochastic Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>As we saw, in each iteration, Batch Gradient Descent does a calculation on the entire\n",
    "        training set, which, for large training sets, may be slow.\n",
    "    </li>\n",
    "    <li><b>Stochastic Gradient Descent (SGD)</b>:\n",
    "        <ul>\n",
    "            <li>On each iteration, it picks just <em>one</em> training example $\\v{x}$ at random and computes \n",
    "                the gradients on just that\n",
    "                one example\n",
    "                $$\\v{\\beta} \\gets \\v{\\beta} - \\alpha\\v{x}^T(\\v{x}\\v{\\beta} - y)$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This gives huge speed-up.</li>\n",
    "    <li>It enables us to train on huge training sets since only one example needs to be in memory in each iteration.\n",
    "    </li>\n",
    "    <li>But, because it is stochastic (the randomness), the loss will not necessarily decrease on each iteration:\n",
    "        <ul>\n",
    "            <li><em>On average</em>, the loss decreases, but in any one iteration, loss may go up or down.</li>\n",
    "            <li>Eventually, it will get close to the minimum, but it will continue to go up and down a bit.\n",
    "                <ul>\n",
    "                    <li>So, once you stop it, the $\\v{\\beta}$ will be close to the best, but not \n",
    "                        necessarily optimal.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>The <code>fit</code> method of scikit-learn's <code>SGDRegressor</code> class is doing\n",
    "        what we have described:\n",
    "        <ul>\n",
    "            <li>You must scale the features but it inserts the extra column of 1s for us.</li>\n",
    "            <li>You can supply a <code>learning_rate</code> and lots of other things\n",
    "                (in the code below, we'll just use the defaults).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>(Again, we'll train on the whole dataset.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create the SGDRegressor and fit the model\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>SGD in numpy</h2>\n",
    "<ul>\n",
    "    <li>For the hell of it, let's implement a simple version ourselves</li>\n",
    "    <li>(Again, we'll train on the whole dataset.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>(One common alternative to the code above is to shuffle between epochs and remove the randomness within the\n",
    "        inner loop.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature-values and the target values \n",
    "X = df[[\"flarea\", \"bdrms\", \"bthrms\"]].values\n",
    "y = df[\"price\"].values\n",
    "\n",
    "# Scale it\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add the extra column to X\n",
    "X = add_dummy_feature(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([338.08076601, 171.72577186,  -6.79513721,  -2.47010611])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression(X, y, alpha = 0.03, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Quite a bumpy ride!</li>\n",
    "    <li>So, let's try <b>simulated annealing</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simulated Annealing</h2>\n",
    "<ul>\n",
    "    <li>As we discussed, SGD does not settle at the minimum.</li>\n",
    "    <li>One solution is to gradually reduce the learning rate:\n",
    "        <ul>\n",
    "            <li>Updates start out 'large' so you make progress.</li>\n",
    "            <li>But, over time, updates get smaller, allowing SGD to settle at or near the global minimum.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The function that determines how to reduce the learning rate is called the <b>learning schedule</b>.\n",
    "        <ul>\n",
    "            <li>Reduce it too quickly and you may not converge on or near to the global minimum.</li>\n",
    "            <li>Reduce it too slowly and you may still bounce around a lot and, if stopped after too few iterations, \n",
    "                may end up\n",
    "                with a suboptimal solution.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    return 5 / (t + 50)\n",
    "    \n",
    "def stochastic_gradient_descent_for_ols_linear_regression_with_simulated_annealing(X, y, num_epochs):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    beta = np.random.randn(n) \n",
    "    Jvals = np.zeros(num_epochs * m)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(m):\n",
    "            rand_idx = np.random.randint(m)\n",
    "            xi = X[rand_idx:rand_idx + 1]\n",
    "            yi = y[rand_idx:rand_idx + 1]\n",
    "            alpha = learning_schedule(epoch * m + i)\n",
    "            beta -= alpha * xi.T.dot(xi.dot(beta) - yi)\n",
    "            Jvals[epoch * m + i] = J(X, y, beta)\n",
    " \n",
    "    return beta, Jvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([353.8406186 , 175.80103636,   2.33197959,   2.04100908])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Stochastic Gradient Descent\n",
    "beta, Jvals = stochastic_gradient_descent_for_ols_linear_regression_with_simulated_annealing(X, y, num_epochs = 50)\n",
    "\n",
    "# Display beta\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAGFCAYAAABtxIBIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfJklEQVR4nO3de7SddX3n8feHBDSiElBK4YAFlWJRKJcj0oXTWmxJxE5BaxXGDtSyZOrdmco0TGeN19ViWV5qR9uiUkFtARWVektTwLaLVS5BLhEUEwELR5BoSLCSiSF854/9iz4cz0kO4WQ/Odnv11rPOs/+/Z7L99knJ/uzn2uqCkmSpF36LkCSJO0YDAWSJAkwFEiSpMZQIEmSAEOBJElqDAWSJAkwFEiSpMZQIEmSAEOBNDKSfCzJux7D/LckecHsVfST5d6Z5Ddme7kzXPd22SZprprfdwGStk2SfYB7gX2r6t7tvb6qevb2Xsew7YzbJD0W7imQ5q7DgdXbOxAkmXNfHuZizdKOwFAgzV2HAzdP15nkyCRfS/LDJBcDj+/0VZJndl4/4tBC26X/x0luBn6UZH53N38bf0uSm5OsS3Jxku7yj0pyQ1v3p1r/jA5dJNkvyWeSrE5yR5I3dvqWJPl2W+6tSV4yg5q3VOdQtkmaKwwF0tx1GNOEgiS7AZ8DPg7sBXwK+J1HufxTgRcDC6vqoSn6Xw4sBg5iEFB+v7PuzwIfa+v+e+AlU8w/Vd27AP8A3ASMAS8E3pxkUZvk28B/AvYA3g58Ism+W6l5yjqnMevbJM0lhgJp7trSnoJjgV2B91fVxqr6NHDdo1z+B6rqrqpav4X+71bVGgYf5Ed01j2/9W+sqkuBa2e4zucCe1fVO6rqx1V1O/Bh4BSAqvpUW+fDVXUxsBI4Zis1T1fnsLZJmjM87ibNQUnmAYcy+EY9lf2AiXrks9G/8yhXc9dW+rvnMjzY1jndure2rM1+AdgvydpO2zzgXwGSnAb8D+DA1vdE4KlbWc90dU5le2yTNGe4p0Cam36RwYflrdP03wOMJUmn7Wmd8QeBJ3Re//wUy6gp2mZiqnUfMMN57wLuqKqFneFJVXVikl9gsNfg9cBTqmoh8HWgu55trXlrHss2SXOGoUCamw4HvlVVG6bp/zfgIeCNSXZN8lIeuZv9RuC/JJmXZDHwa7NY278Bm4DXt5P9Tpq07i25FvhhO2FwQavvOUmeC+zO4EN/NUCSVwHPmcW6t+SxbJM0ZxgKpLnpMKY/dEBV/Rh4KYMT5dYArwAu7UzyJuA/A2uBVzI4KXFWdNZ9Rlv+7wFfAKYLMN15NwG/xeBY/h3A94GPAHtU1a3Aexh8QH+PwXtw1WzVvZW6tnmbpLkkjzxEJmkuSHIl8HdV9eG+a5mJJNcAf11Vf9t3LbNlZ9wmyT0F0hyT5DcZfEv+bN+1TCfJryX5+bar/XQGhzu+0nddj8XOuE3SZEMNBe3mICuS3JhkeWvbK8myJCvbzz1be5J8IMmqdjORozrLOb1Nv7L9cW5uP7otf1WbNz9bhTR3JVkBnAu8rKq+33c9W3AIg8Mba4E/YlDvPb1W9NjtjNskPcJQDx8kuRMY7/5nluTPgTVVdU6SJcCeVfXHSU4E3gCcCDwP+Iuqel6SvYDlwDiDk46uB46uqvuTXAu8EbgG+BKDa4q/PLQNlCRpDtsRDh+cBFzQxi8ATu60X1gDVwML253LFgHLqmpNVd0PLAMWt74nV9XV7VriCzvLkiRJWzHsUFDAPya5PsmZrW2fzi64e4F92vgYj7w5yN2tbUvtd0/RLkmSZmDYdzR8flVNJPk5YFmSb3Y7q6qSbPfjGS2QnAmw++67H/2sZz1re69SkqQdwvXXX//9qtp7qr6hhoKqmmg/70vyWQY3//hekn2r6p52COC+NvkEj7xj2P6tbQJ4waT2r7b2/aeYfqo6zgPOAxgfH6/ly5c/tg2TJGmOSDLtLc+Hdvggye5JnrR5HDiBwS1KLwM2X0FwOvD5Nn4ZcFq7CuFYYF07zLAUOCHJnu1KhROApa3vgSTHtqsOTussS5IkbcUw9xTsA3y2XSU4n8GNV76S5DrgkiRnMHhgy8vb9F9icOXBKgb3aX8VQFWtSfJOfvrEt3e0J5oBvJbBo00XAF9ugyRJmoGRv6Ohhw8kSaMkyfVVNT5V345wSaIkSdoBGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUGAokSRJgKJAkSY2hQJIkAYYCSZLUDD0UJJmX5IYkX2ivD0pyTZJVSS5Osltrf1x7var1H9hZxtmt/bYkizrti1vbqiRLhr1tkiTNZX3sKXgT8I3O63cD76uqZwL3A2e09jOA+1v7+9p0JDkUOAV4NrAY+FALGvOADwIvAg4FTm3TSpKkGRhqKEiyP/Bi4CPtdYDjgU+3SS4ATm7jJ7XXtP4XtulPAi6qqg1VdQewCjimDauq6vaq+jFwUZtWkiTNwLD3FLwf+J/Aw+31U4C1VfVQe303MNbGx4C7AFr/ujb9T9onzTNduyRJmoGhhYIkvwXcV1XXD2udW6jlzCTLkyxfvXp13+VIkrRDGOaeguOA305yJ4Nd+8cDfwEsTDK/TbM/MNHGJ4ADAFr/HsAPuu2T5pmu/WdU1XlVNV5V43vvvfdj3zJJknYCQwsFVXV2Ve1fVQcyOFHwiqp6JXAl8LI22enA59v4Ze01rf+KqqrWfkq7OuEg4GDgWuA64OB2NcNubR2XDWHTJEnaKczf+iTb3R8DFyV5F3AD8NHW/lHg40lWAWsYfMhTVbckuQS4FXgIeF1VbQJI8npgKTAPOL+qbhnqlkiSNIdl8OV7dI2Pj9fy5cv7LkOSpKFIcn1VjU/V5x0NJUkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAlZMrOO4c67gczdM9F2KJEm9GvlQADCxdj1nX7rCYCBJGmmGgmb9xk2cu/S2vsuQJKk3hoKO765d33cJkiT1xlDQsd/CBX2XIElSbwwFzYJd53HWokP6LkOSpN7M77uAHcHYwgWctegQTj5yrO9SJEnqzciHgsPG9uCqJcf3XYYkSb3z8IEkSQIMBZIkqTEUSJIkwFAgSZIaQ4EkSQIMBZIkqTEUSJIkwFAgSZIaQ4EkSQIMBZIkqTEUSJIkwFAgSZIaQ4EkSQIMBZIkqRlaKEjy+CTXJrkpyS1J3t7aD0pyTZJVSS5Osltrf1x7var1H9hZ1tmt/bYkizrti1vbqiRLhrVtkiTtDIa5p2ADcHxV/TJwBLA4ybHAu4H3VdUzgfuBM9r0ZwD3t/b3telIcihwCvBsYDHwoSTzkswDPgi8CDgUOLVNK0mSZmBooaAG/qO93LUNBRwPfLq1XwCc3MZPaq9p/S9MktZ+UVVtqKo7gFXAMW1YVVW3V9WPgYvatJIkaQaGek5B+0Z/I3AfsAz4NrC2qh5qk9wNjLXxMeAugNa/DnhKt33SPNO1S5KkGRhqKKiqTVV1BLA/g2/2zxrm+jdLcmaS5UmWr169uo8SJEna4fRy9UFVrQWuBH4FWJhkfuvaH5ho4xPAAQCtfw/gB932SfNM1z7V+s+rqvGqGt97771nY5MkSZrzhnn1wd5JFrbxBcBvAt9gEA5e1iY7Hfh8G7+svab1X1FV1dpPaVcnHAQcDFwLXAcc3K5m2I3ByYiXbfcNkyRpJzF/65PMmn2BC9pVArsAl1TVF5LcClyU5F3ADcBH2/QfBT6eZBWwhsGHPFV1S5JLgFuBh4DXVdUmgCSvB5YC84Dzq+qW4W2eJElzWwZfvkfX+Ph4LV++vO8yJEkaiiTXV9X4VH3e0VCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAGGAkmS1BgKJEkSYCiQJEmNoUCSJAEwf2sTJHkvcHMbbqmqDdu9KkmSNHRbDQXAKuBY4NXALyW5l5+GhOuAfzEoSJI09201FFTVh7qvkxwEHAYcDrwG+Jskr6mqpdunREmSNAwz2VPwCFV1B3AHcBlAkn2BLwCGAkmS5rDHfKJhVd0D/N0s1CJJkno0K1cfVNV7ZmM5kiSpP16SKEmSAEOBJElqDAWSJAkwFEiSpMZQIEmSAEOBJElqRj4UrJhYx3HnXMHnbpjouxRJkno18qEAYGLtes6+dIXBQJI00gwFzfqNmzh36W19lyFJUm8MBR3fXbu+7xIkSeqNoaBjv4UL+i5BkqTeGAqaBbvO46xFh/RdhiRJvXnUj07eGY0tXMBZiw7h5CPH+i5FkqTejHwoOGxsD65acnzfZUiS1DsPH0iSJMBQIEmSGkOBJEkCDAWSJKkxFEiSJMBQIEmSGkOBJEkCDAWSJKkxFEiSJMBQIEmSGkOBJEkCDAWSJKkxFEiSJMBQIEmSGkOBJEkCDAWSJKkxFEiSJMBQIEmSGkOBJEkCDAWsmFjHcedcwedumOi7FEmSejXyoQBgYu16zr50hcFAkjTSDAXN+o2bOHfpbX2XIUlSbwwFHd9du77vEiRJ6o2hoGO/hQv6LkGSpN4YCpoFu87jrEWH9F2GJEm9GVooSHJAkiuT3JrkliRvau17JVmWZGX7uWdrT5IPJFmV5OYkR3WWdXqbfmWS0zvtRydZ0eb5QJLMpLaxhQv4s5cexslHjs32ZkuSNGcMc0/BQ8AfVdWhwLHA65IcCiwBLq+qg4HL22uAFwEHt+FM4K9gECKAtwLPA44B3ro5SLRpXt2Zb/HWijpsbA+uWnK8gUCSNPKGFgqq6p6q+lob/yHwDWAMOAm4oE12AXByGz8JuLAGrgYWJtkXWAQsq6o1VXU/sAxY3PqeXFVXV1UBF3aWJUmStqKXcwqSHAgcCVwD7FNV97Sue4F92vgYcFdntrtb25ba756ifar1n5lkeZLlq1evfmwbI0nSTmLooSDJE4HPAG+uqge6fe0bfm3vGqrqvKoar6rxvffee3uvTpKkOWGooSDJrgwCwSer6tLW/L2265/2877WPgEc0Jl9/9a2pfb9p2iXJEkzMMyrDwJ8FPhGVb2303UZsPkKgtOBz3faT2tXIRwLrGuHGZYCJyTZs51geAKwtPU9kOTYtq7TOsuSJElbMX+I6zoO+K/AiiQ3trb/BZwDXJLkDOA7wMtb35eAE4FVwIPAqwCqak2SdwLXteneUVVr2vhrgY8BC4Avt0GSJM1ABofxR9f4+HgtX7687zIkSRqKJNdX1fhUfd7RUJIkAYYCSZLUjHwoWDGxjuPOuYLP3eCFCpKk0TbyoQBgYu16zr50hcFAkjTSDAXN+o2bOHfpbX2XIUlSbwwFHd9du77vEiRJ6o2hoGPhE3btuwRJknpjKOgY8Vs2SJJGnKGgY+36jX2XIElSbwwFkiQJMBRIkqTGUCBJkgBDgSRJagwFHWMLF/RdgiRJvTEUNAt2ncdZiw7puwxJknozv+8CdgRjCxdw1qJDOPnIsb5LkSSpNyMfCg4b24OrlhzfdxmSJPXOwweSJAkwFEiSpMZQIEmSAEOBJElqRj4UrJhYx3HnXMHnbpjouxRJkno18qEAYGLtes6+dIXBQJI00gwFzfqNmzh36W19lyFJUm8MBR3fXbu+7xIkSeqNoaBjjwW79l2CJEm9MRR0JH1XIElSfwwFHWsf3Nh3CZIk9cZQ0LGfj06WJI0wQ0Hjo5MlSaNu5J+SCD46WZIkMBT46GRJkpqRP3zgbY4lSRoY+VAA3uZYkiQwFPyEtzmWJI06Q0GHtzmWJI0yQ0GHtzmWJI0yQ0HHjzZ4R0NJ0ugyFHRsfLjvCiRJ6o+hQJIkAYYCSZLUGAokSRJgKHiE3Xeb13cJkiT1xlDQ8ZKjfCCSJGl0GQo6rvzm6r5LkCSpN4aCDu9oKEkaZYaCDu9oKEkaZYaCjo2bvHuRJGl0GQo6fvTjTX2XIElSbwwFkiQJMBRIkqTGUCBJkgBDgSRJagwFHQu9JFGSNMIMBc2uu4S3/faz+y5DkqTezO+7gB3B2MIFnLXoEE4+0mcfSJJG18iHgsPG9uCqJcf3XYYkSb3z8IEkSQIMBZIkqTEUSJIkwFAgSZKaoYWCJOcnuS/J1ztteyVZlmRl+7lna0+SDyRZleTmJEd15jm9Tb8yyemd9qOTrGjzfCBJhrVtkiTtDIa5p+BjwOJJbUuAy6vqYODy9hrgRcDBbTgT+CsYhAjgrcDzgGOAt24OEm2aV3fmm7yuKX3z3h9y0JIvctw5V/C5Gya2bcskSdoJDC0UVNW/AGsmNZ8EXNDGLwBO7rRfWANXAwuT7AssApZV1Zqquh9YBixufU+uqqurqoALO8vaoo2bHqaAibXrOevTNxkMJEkjq+9zCvapqnva+L3APm18DLirM93drW1L7XdP0T6lJGcmWZ5k+aYH1/2kfeOm4u3/cMs2bookSXNb36HgJ9o3/BrSus6rqvGqGp/3hD0e0Xf/gxuHUYIkSTucvkPB99quf9rP+1r7BHBAZ7r9W9uW2vefol2SJM1Q36HgMmDzFQSnA5/vtJ/WrkI4FljXDjMsBU5Ismc7wfAEYGnreyDJse2qg9M6y3pUvGRBkjSqhvbsgyR/D7wAeGqSuxlcRXAOcEmSM4DvAC9vk38JOBFYBTwIvAqgqtYkeSdwXZvuHVW1+eTF1zK4wmEB8OU2PGpDOX4hSdIOaGihoKpOnabrhVNMW8DrplnO+cD5U7QvB57zWGoE9xRIkkZX34cPdjjuKZAkjSpDgSRJAgwFkiSpMRRIkiTAUCBJkhpDgSRJAgwFkiSpMRRIkiTAUCBJkhpDgSRJAgwFP+P3jn1a3yVIktQLQ8Ek7zr5sL5LkCSpF4aCjoN/bve+S5AkqTeGAkmSBBgKHmHlfT/iN9/71b7LkCSpF4aCSVbe9yMOf+tX+i5DkqShMxRM4YENmwwGkqSRYyiYxgMbNvVdgiRJQ2UokCRJgKFAkiQ1hgJJkgQYCiRJUmMokCRJgKFgi7wsUZI0SgwFW+BliZKkUWIokCRJgKFAkiQ1hoKtOHDJFz23QJI0EgwFM+CzECRJo2B+3wXMFQ9s2MSBS77IvIRTn3cA7zr5sL5LkiRpVhkKHqVNVXzi6n/nE1f/OwB3nvPiniuSJGl2ePjgMTpwyRf7LkGSpFkx8nsKDhvbg+8/xmUcuOSL3HnOizn8rV+Z9t4G7lGQJO3oRj4UzJat7THYHBz6sC17MwwxkjR6DAVDNJcONcylWqW+bC08T/d3tLX5nnn2F3mofvp6fnjE61ExG19O5sL/Ze9/xRG8+eIbZ3WZm9+7qbZ/t59/5tHTzZeqEfyX1jE+Pl7f/423912GJElDcc8Fb2bDPSszVZ8nGkqSJMBQAMCUcUmSpBFjKADu8KQ6SZIMBZt5tr0kadSN/ImGSVYD35mqb0tnaGp2bHpwHfOesEffZYws3/9++f73a1Tf/4fW3cemB9dNeeR85EOB+pVkeVWN913HqPL975fvf798/3+Whw8kSRJgKJAkSY2hQH07r+8CRpzvf798//vl+z+J5xRIkiTAPQWSJKkxFGjWJbkzyYokNyZZ3tr2SrIsycr2c8/WniQfSLIqyc1Jjuos5/Q2/cokp/e1PTu6JOcnuS/J1ztts/Z+Jzm6/T5XtXm9Cegk0/wO3pZkov0d3JjkxE7f2e39vC3Jok774ta2KsmSTvtBSa5p7Rcn2W14W7djS3JAkiuT3JrkliRvau3+DWyLqnJwmNUBuBN46qS2PweWtPElwLvb+InAlxncbfpY4JrWvhdwe/u5Zxvfs+9t2xEH4FeBo4Cvb4/3G7i2TZs274v63uYdbZjmd/A24C1TTHsocBPwOOAg4NvAvDZ8G3g6sFub5tA2zyXAKW38r4HX9L3NO8oA7Asc1cafBHyrvcf+DWzD4J4CDctJwAVt/ALg5E77hTVwNbAwyb7AImBZVa2pqvuBZcDiIdc8J1TVvwBrJjXPyvvd+p5cVVfX4H/HCzvLUjPN72A6JwEXVdWGqroDWAUc04ZVVXV7Vf0YuAg4qX0rPR74dJu/+/sceVV1T1V9rY3/EPgGMIZ/A9vEUKDtoYB/THJ9kjNb2z5VdU8bvxfYp42PAXd15r27tU3XrpmZrfd7rI1PbtfMvL7toj5/8+5rHv3v4CnA2qp6aFK7JklyIHAkcA3+DWwTQ4G2h+dX1VHAi4DXJfnVbmdL2172MiS+3735K+AZwBHAPcB7eq1mJ5fkicBngDdX1QPdPv8GZs5QoFlXVRPt533AZxnsFv1e2w1H+3lfm3wCOKAz+/6tbbp2zcxsvd8TbXxyu7aiqr5XVZuq6mHgwwz+DuDR/w5+wGAX9/xJ7WqS7MogEHyyqi5tzf4NbANDgWZVkt2TPGnzOHAC8HXgMmDz2bynA59v45cBp7Uzgo8F1rVdfkuBE5Ls2Xa7ntDaNDOz8n63vgeSHNuObZ/WWZa2YPMHUvMSBn8HMPgdnJLkcUkOAg5mcCLbdcDB7UqD3YBTgMvat9wrgZe1+bu/z5HX/l1+FPhGVb230+XfwLbo+0xHh51rYHDm9E1tuAX4k9b+FOByYCXwT8BerT3ABxmcdb0CGO8s6w8YnIS1CnhV39u2ow7A3zPYPb2RwfHOM2bz/QbGGXygfRv4v7Sbnjls9Xfw8fYe38zgg2jfzvR/0t7P2+icyc7gzPhvtb4/6bQ/nUFwWAV8Cnhc39u8owzA8xkcGrgZuLENJ/o3sG2DdzSUJEmAhw8kSVJjKJAkSYChQJIkNYYCSZIEGAokSVJjKJDmkCSV5D2d129J8rZZWvbHkrxs61M+5vX8bpJvJLlyUvt+ST7dxo/oPlVwFta5MMlrp1qXpJ8yFEhzywbgpUme2nchXZ277c3EGcCrq+rXu41V9d2q2hxKjmBwrfls1bAQ+EkomLQuSY2hQJpbHgLOA/775I7J3/ST/Ef7+YIk/5zk80luT3JOklcmubY9I/4ZncX8RpLlSb6V5Lfa/POSnJvkuvZwn//WWe6/JrkMuHWKek5ty/96kne3tv/D4GYzH01y7qTpD2zT7ga8A3hFkhuTvKLdKfP8VvMNSU5q8/x+ksuSXAFcnuSJSS5P8rW27pPa4s8BntGWd+7mdbVlPD7J37bpb0jy651lX5rkK0lWJvnzzvvxsVbriiQ/87uQ5qpHk+4l7Rg+CNy8+UNqhn4Z+CUGj/e9HfhIVR2T5E3AG4A3t+kOZHCP/mcAVyZ5JoPbuq6rqucmeRxwVZJ/bNMfBTynBo8A/okk+wHvBo4G7mfw1MyTq+odSY4H3lJVy6cqtKp+3MLDeFW9vi3vT4ErquoPkiwErk3yT50aDq+qNW1vwUuq6oG2N+XqFlqWtDqPaMs7sLPK1w1WW4cleVar9Rdb3xEMnrq3AbgtyV8CPweMVdVz2rIWbuF9l+YU9xRIc0wNngB3IfDGRzHbdTV47vwGBrdq3fyhvoJBENjskqp6uKpWMggPz2JwD/jTktzI4JG0T2Fwv36AaycHgua5wFeranUNHvn7SeBXp5hupk4AlrQavgo8Hnha61tWVWvaeIA/TXIzg1vbjvHTR+ZO5/nAJwCq6pvAd4DNoeDyqlpXVf+Pwd6QX2Dwvjw9yV8mWQw8MMUypTnJPQXS3PR+4GvA33baHqIF/SS7ALt1+jZ0xh/uvH6YR/4/MPm+58Xgg/YNVfWIB1IleQHwo20pfhsE+J2qum1SDc+bVMMrgb2Bo6tqY5I7GQSIbdV93zYB86vq/iS/DCwC/hB4OYN75ktznnsKpDmofTO+hMFJe5vdyWB3PcBvA7tuw6J/N8ku7TyDpzN4YM9S4DUZPJ6WJL+YwRMwt+Ra4NeSPDXJPOBU4J8fRR0/BJ7Ueb0UeEN7Sh1Jjpxmvj2A+1og+HUG3+ynWl7XvzIIE7TDBk9jsN1TaocldqmqzwD/m8HhC2mnYCiQ5q73AN2rED7M4IP4JuBX2LZv8f/O4AP9y8Aftt3mH2Gw6/xr7eS8v2Erexlr8LjZJQwe+XsTcH1VPZrHzV4JHLr5REPgnQxCzs1Jbmmvp/JJYDzJCgbnQnyz1fMDBudCfH3yCY7Ah4Bd2jwXA7/fDrNMZwz4ajuU8Qng7EexXdIOzackSpIkwD0FkiSpMRRIkiTAUCBJkhpDgSRJAgwFkiSpMRRIkiTAUCBJkhpDgSRJAuD/A+IDULTQgvMqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.title(\"$J$ during learning\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.xlim(1, Jvals.size)\n",
    "plt.ylabel(\"$J$\")\n",
    "plt.ylim(3500, 50000)\n",
    "xvals = np.linspace(1, Jvals.size, Jvals.size)\n",
    "plt.scatter(xvals, Jvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Mini-Batch Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Batch Gradient Descent computes gradients from the full training set.</li>\n",
    "    <li>Stochastic Gradient Descent computes gradients from just one example.</li>\n",
    "    <li>Mini-Batch Gradient Descent lies between the two:\n",
    "        <ul>\n",
    "            <li>It computes gradients from a small randomly-selected subset of the training set, called a\n",
    "                <b>mini-batch</b>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Since it lies between the two:\n",
    "        <ul>\n",
    "            <li>It may bounce less and get closer to the global minimum than SGD&hellip;\n",
    "                <ul>\n",
    "                    <li>&hellip;although both of them can reach the global minimum with a good learning schedule.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Its time and memory costs lie between the two.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Normal Equation versus Gradient Descent</h1>\n",
    "<ul>\n",
    "    <li>Efficiency/scaling-up to large training sets:\n",
    "        <ul>\n",
    "            <li>Normal Equation: \n",
    "                <ul>\n",
    "                    <li>is linear in $m$, so can handle large training sets efficiently if they fit into\n",
    "                        main memory;\n",
    "                    </li>\n",
    "                    <li>but it has to compute the inverse (or psueudo-inverse) of a $n \\times n$ matrix, which takes\n",
    "                        time between quadratic and cubic in $n$, and so is only feasible for smallish $n$ (up to\n",
    "                        a few thousand).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Gradient Descent:\n",
    "                <ul>\n",
    "                    <li>SGD scales really well to huge $m$;</li>\n",
    "                    <li>All three Gradient Descent methods can handle huge $n$ (even 100s of 1000s).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finding the global minimum for OLS regression:\n",
    "        <ul>\n",
    "            <li>Normal Equation: guaranteed to find the global minimum.</li>\n",
    "            <li>Gradient Descent: all a bit dependent on number of iterations, learning rate, learning schedule.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Feature scaling:\n",
    "        <ul>\n",
    "            <li>Normal Equation: scaling is not needed. \n",
    "            </li>\n",
    "            <li>Gradient Descent: scaling <em>is</em> needed.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Finally, Gradient Descent is a general method, whereas the Normal Equation is only for OLS regression.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Non-Convex Functions</h1>\n",
    "<ul>\n",
    "    <li>The loss function for OLS regression is convex and it has a slope that never changes abruptly.\n",
    "        <ul>\n",
    "            <li>This gives us good 'guarantees' about reaching the minimum\n",
    "                (depending on such things as running for long enough, using a learning rate that isn't too high,\n",
    "                and whether we are using Batch, Mini-Batch or Stochastic Gradient Descent).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But Gradient Descent is a generic method: you can use it to find the minima of other loss functions.</li>\n",
    "    <li>But not all loss functions are convex, which can cause problems for Gradient Descent:\n",
    "        <figure>\n",
    "            <img src=\"images/local_minima.png\" />\n",
    "        </figure>\n",
    "        <ul>\n",
    "            <li>The algorithm might converge to a local minimum, instead of the global minimum.</li>\n",
    "            <li>It may take a long time to cross a plateau.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What do we do about this?\n",
    "        <ul>\n",
    "            <li>One thing is to prefer Stochastic Gradient Descent (or Mini-Batch Gradient Descent):\n",
    "                because of the way they 'bounce around', they might even escape a\n",
    "                local minimum, and might even get to the global minimum.\n",
    "            </li>\n",
    "            <li>In this context, simulated annealing is also useful: updates start out 'large' allowing these\n",
    "                algorithms to make \n",
    "                progress and even escape local minima; but, over time, updates get smaller, allowing \n",
    "                these algorithms to settle at or near the global minimum.\n",
    "            </li>\n",
    "            <li>But, if using simulated annealing, if you reduce the learning rate too quickly, you may \n",
    "                stil get stuck in a local minimum.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
