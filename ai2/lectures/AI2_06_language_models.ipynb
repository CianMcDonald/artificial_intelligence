{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Language Models</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "     <li>The code comes from: \n",
    "        A. G&eacute;ron: \n",
    "        <i>Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Warning</h1>\n",
    "<ul>\n",
    "    <li>The code takes a little time to run.\n",
    "    </li>\n",
    "    <li>It is not important to understand this code in any case.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Natural Language Processing</h1>\n",
    "<ul>\n",
    "    <li>In the previous lectures, we looked at a task (sentiment analysis) that requires <i>natural language\n",
    "        understanding</i>. We tried various approaches including word embeddings and recurrent neural networks.\n",
    "    </li>\n",
    "    <li>In this lecture, we look at  <i>language models</i> and use them for \n",
    "        <i>natural language generation</i> &mdash; producing language.</li>\n",
    "    <li>Specifically, \n",
    "        <ul>\n",
    "            <li>we build a model from training data that can predict the next character in a sentence;</li>\n",
    "            <li>we then use that model, with a bit of randomization, to produce new sentences in the style of\n",
    "                the original training data.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We do this at the level of individual characters, but it can be done at a higher-level, e.g.\n",
    "        predicting/generating the next word.\n",
    "    </li>\n",
    "    <li>This might seem frivolous, but it gives insight into a number of useful systems that we will\n",
    "        mention at the end of the lecture.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Char-RNN</h1>\n",
    "<ul>\n",
    "    <li>Everyone does this on Shakespeare &mdash; perhaps because if it outputs bad\n",
    "        Shakespeare some people still think it sounds like Shakespeare!\n",
    "    </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing the training data</h2>\n",
    "<ul>\n",
    "    <li>Most of the effort goes into preprocessing the dataset. Don't get bogged down in the details of this code.</li>\n",
    "    <li>We're one-hot encoding the characters.</li>\n",
    "    <li>We're making overlapping windows, shuffling these, and putting them into batches.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you the first part of it\n",
    "shakespeare_text[:148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you all its distinct characters\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a character-level (rather than word-level) tokenizer\n",
    "# In effect, it lowercases and assigns ids to characters from 1 to 39 inc, e.g. ' ' is 1, 'e' is 2, etc.\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 23, 2, 5, 25]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you an encoding\n",
    "tokenizer.texts_to_sequences([\"speak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s p e a k']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you the reverse\n",
    "tokenizer.sequences_to_texts([[8, 23, 2, 5, 25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the full text (subtract 1 to use ids from 0 to 38 instead of 1 to 39, so now ' ' is 0, 'e' is 1, etc.)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train on the whole dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, we can't train on the whole training set in one go: too long\n",
    "# window() splits this into smaller windows of text\n",
    "# Using shift=1 means the first window is characters 0 to 100, the second is characters 1 to 101, etc.\n",
    "# Using drop_remainder=True means all windows are 101 characters long witout needing us to pad the\n",
    "# last ones (they are dropped)\n",
    "# But window() produces a nested dataset: a dataset containing windows (each of which is a dataset)\n",
    "# so we flatten it\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # to include the target\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the windows and put into batches\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the inputs (the first 100 characters) from the targets (the last, i.e. 101st, character)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment, characters are replaced by ids. Now we one-hot encode them.\n",
    "# (This is OK for Char-RNN. If we were doing something with words, we might use word embeddings)\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching means while training on one batch, the next is being prepared\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Show you the shape of the first batch\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Char-RNN Model</h2>\n",
    "<ul>\n",
    "    <li>The input shape is <code>[None, max_id]</code> because of the one-hot encoding.</li>\n",
    "    <li>We'll use a couple of GRU layers with dropout on their inputs and their hidden state.</li>\n",
    "    <li>The output layer has <code>max_id</code> neurons, because we're predicting that number of\n",
    "        distinct characters, i.e. we have <code>max_id</code> classes.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_rnn():\n",
    "    network = Sequential()\n",
    "    network.add(GRU(128, return_sequences=True, activation=\"tanh\",\n",
    "                    dropout=0.2, recurrent_dropout=0.2, input_shape=[None, max_id]))\n",
    "    network.add(GRU(128, return_sequences=True, activation=\"tanh\",\n",
    "                    dropout=0.2, recurrent_dropout=0.2))\n",
    "    network.add(Dense(max_id, activation=\"softmax\"))\n",
    "    network.compile(optimizer=RMSprop(lr=0.0003), loss=\"sparse_categorical_crossentropy\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn = build_char_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "history = char_rnn.fit(dataset, \n",
    "                       steps_per_epoch=dataset_size // batch_size // n_steps,\n",
    "                       epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictions using Char-RNN</h2>\n",
    "<ul>\n",
    "    <li>Given some text (suitably preprocessed), the model can predict the next character\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to preprocess the text whose next character we will predict: tokenize and one-hot encode\n",
    "def preprocess(text):\n",
    "    X = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = preprocess([\"How are yo\"]) \n",
    "prediction = char_rnn.predict_classes(input_text)\n",
    "tokenizer.sequences_to_texts(prediction + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating Text using Char-RNN</h2>\n",
    "<ul>\n",
    "    <li>To generate text, we want to make repeated predictions:\n",
    "        <ul>\n",
    "            <li>Feed in some initial input;</li>\n",
    "            <li>Predict the most likely next character;</li>\n",
    "            <li>Add the prediction to the end of the input text;</li>\n",
    "            <li>Feed in the extended input;</li>\n",
    "            <li>Predict the most likely next character;</li>\n",
    "        </ul>\n",
    "        and so on.\n",
    "    </li>\n",
    "    <li>But this leads to repetitive text.</li>\n",
    "    <li>Instead, we make it stochastic:\n",
    "        <ul>\n",
    "            <li>We pick the next character randomly but based on the probabilities that the network produces.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The temperature parameter allows you to tune it: \n",
    "# - a value close to zero favours high probability characters, but leads to more repetition\n",
    "# - a high value gives all characters an almost equal probability\n",
    "def next_char(model, text, temperature=1):\n",
    "    X = preprocess([text])\n",
    "    y_proba = model.predict(X)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def generate_text(model, text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temperature:\n",
      " ther and servingman:\n",
      "and servingman:\n",
      "and the cansel \n",
      "\n",
      "Medium temperature:\n",
      " thhir before.\n",
      "\n",
      "ferem is is it lead mestisead.\n",
      "\n",
      "cori \n",
      "\n",
      "High temperature:\n",
      " tyou tgy k;\n",
      "iunn back, nve i'ly ara-hhveld?--elain. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some examples\n",
    "print(\"Low temperature:\\n\", generate_text(char_rnn, \"t\", temperature=0.2), '\\n')\n",
    "\n",
    "print(\"Medium temperature:\\n\", generate_text(char_rnn, \"t\", temperature=1), '\\n')\n",
    "\n",
    "print(\"High temperature:\\n\", generate_text(char_rnn, \"t\", temperature=2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>How can we make the generated text more convincing?\n",
    "        <ul>\n",
    "            <li>Tweak everything! More layers, more neurons per layer, more epochs, &hellip;\n",
    "            <li>You could make the windows bigger by increasing <code>n_steps</code> but even LSTM and GRUs,\n",
    "                while better than SimpleRNNs, cannot handle very long sequences.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We could change Char-RNN from being a <b>stateless RNN</b> to being a <b>stateful RNN</b>.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stateless RNNs and Stateful RNNs</h2>\n",
    "<ul>\n",
    "    <li><b>Stateless RNN:</b> In a training iteration, \n",
    "        <ul>\n",
    "            <li>will be trained on a batch of random chunks of the text;</li>\n",
    "            <li>hidden state starts at all zeros;</li>\n",
    "            <li>processes the input, step by step;</li>\n",
    "            <li>after the last timestep, throws away the hidden state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Stateful RNN:</b>\n",
    "        <ul>\n",
    "            <li>preserve the hidden state at the end of the last timestep;</li>\n",
    "            <li>use it as the initial hidden state for the next batch.</li>\n",
    "        </ul>\n",
    "        This way, we can learn longer patterns despite only back-propagating through short\n",
    "        sequences.\n",
    "    </li>\n",
    "    <li>However, we now must arrange our batches quite carefully.\n",
    "        <ul>\n",
    "            <li>Each input sequence in a batch starts where the corresponding sequence in the previous\n",
    "                batch finished.\n",
    "            </li>\n",
    "            <li>In other words, we must remove the overlapping and the shuffling that we used in the\n",
    "                stateless RNN.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Keras comes with a parameter for its recurrent layers, <code>stateful=True</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applications</h1>\n",
    "<ul>\n",
    "    <li><i>Sunspring</i> is a sci-fi movie whose script was generated by an LSTM trained on existing\n",
    "        movie scripts: <a href=\"http://www.thereforefilms.com/sunspring.html\">http://www.thereforefilms.com/sunspring.html</a>\n",
    "    </li>\n",
    "    <li>We can generate music in this way too, e.g. <a href=\"https://folkrnn.org/\">https://folkrnn.org/</a></li>\n",
    "    <li>There are researchers who are trying to generate explanations using these techniques.</li>\n",
    "    <li>But let's look briefly at image captioning, machine translation and question-answering.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Image Captioning</h2>\n",
    "<ul>\n",
    "    <li>Google have learned a model for automatically captioning images.</li>\n",
    "    <li>It is trained on a dataset of images plus human captions.</li>\n",
    "    <li>The neural network combines a convolutional network to find features in the image\n",
    "        (using the Inception V3 model) but also a recurrent neural network that predicts\n",
    "        words, similar to what we have just studied.\n",
    "        <figure>\n",
    "            <img src=\"images/captioning.png\" />\n",
    "            <figcaption>\n",
    "                Google's image captioning system<br /> See\n",
    "                <a href=\"https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html\">https://research.googleblog.com/2016/09/show-and-tell-image-captioning-open.html</a><br />\n",
    "                Image comes from Vinyals et al.: <i>Show and Tell: Lessons learned from the \n",
    "                2015 MSCOCO Image Captioning Challenge</i>, CoRR, abs/1609.06647, 2016 \n",
    "                (<a href=\"https://arxiv.org/pdf/1609.06647.pdf\">https://arxiv.org/pdf/1609.06647.pdf</a>)\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>There are many details, which we will ignore.\n",
    "        <ul>\n",
    "            <li>Handling training sentences of various lengths (we can pad but we can't crop; common\n",
    "                instead is for batches to contain sentences of the same length).\n",
    "            </li>\n",
    "            <li>After the network predicts the end-of-sentence token, any further outputs should be\n",
    "                ignored and not contribute to the loss.\n",
    "            </li>\n",
    "            <li>This is multi-classs classification for an enormous number of classes (one per English\n",
    "                word); even computing softmax will be slow; we can speed this up during training (not \n",
    "                afterwards) with sampled softmax, so that the loss is only approximated.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Take a look at this <a href==\"https://twitter.com/JanelleCShane/status/969239712190746624?s=19\">Twitter thread about sheep</a>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Machine Translation</h2>\n",
    "<ul>\n",
    "    <li>Datasets of parallel text are often available, e.g. proceedings of the Canadian parliament\n",
    "        are published in both English and French. These give our training set.\n",
    "        <ul>\n",
    "            <li>The words may be assigned ids and these may be one-hot encoded.</li>\n",
    "            <li>Or we might include a word embedding layer (not shown below).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>During training, at each timestep, we feed a word of the English sentence into the encoder\n",
    "        and the previous word of the French sentence into the decoder.\n",
    "        The decoder is predicting the next word. If its prediction is incorrect, weights will get changed.\n",
    "     </li>\n",
    "     <li>After training, at each timestep, we feed a word of the English sentence that is to be\n",
    "         translated into the encoder and the previous <em>predicted</em> French word into the decoder.\n",
    "    </li>\n",
    "    <li>There are many details, many of them are the same as the ones we described for image captioning. But\n",
    "        new details too, e.g.\n",
    "        <ul>\n",
    "            <li>We feed in the English sentence in reverse.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>There are also many new ideas, which we do not have time to cover. Look them up if interested, e.g.:\n",
    "        <ul>\n",
    "            <li>Bidirectional RNNs;</li>\n",
    "            <li>Using beam search for a less greedy solution;</li>\n",
    "            <li>Attention mechanisms:\n",
    "                <ul>\n",
    "                    <li>These have had a huge effect.</li>\n",
    "                    <li>Instead of feeding just the last encoder output into the decoder, we feed in \n",
    "                        all outputs.\n",
    "                     </li>\n",
    "                    <li>The attention mechanism is, e.g., a weighted sum of these encoder outputs.</li>\n",
    "                    <li>These weights determine which encoder output (and hence, in some sense, which\n",
    "                        word of the English input) it will focus on at this step.\n",
    "                    </li>\n",
    "                    <li>How are these weights learned? The attention mechanism has a small neural network\n",
    "                        called an attention layer, which learns them.\n",
    "                    </li>\n",
    "                </ul>\n",
    "                Attention mechanisms have been added to image captioning systems and there are even\n",
    "                neural networks (such as the Tranformer) which use attention layers in place of recurrent layers\n",
    "                and convolutional layers.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<figure>\n",
    "    <img src=\"images/translation.png\" />\n",
    "    <figcaption>\n",
    "        Google's Machine Translation system<br /> See\n",
    "        <a href=\"https://research.googleblog.com/2016/09/a-neural-network-for-machine.html\">https://research.googleblog.com/2016/09/a-neural-network-for-machine.html</a><br />\n",
    "        Image comes from Wu et al.: <i>Google's Neural Machine Translation System: Bridging the Gap between\n",
    "        Human and Machine Translation</i>, CoRR, abs/1609.08144, 2016\n",
    "        (<a href=\"https://arxiv.org/pdf/1609.08144.pdf\">https://arxiv.org/pdf/1609.08144.pdf</a>)\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Question-Answering</h2>\n",
    "<ul>\n",
    "    <li>AI has long had the goal of producing a question-answering system, especially one that\n",
    "        can hold extended conversations &mdash; it's central to the Turing Test, for example.\n",
    "    </li>\n",
    "    <li>Traditional chatbots can just about function in very narrow domains but they fail as soon\n",
    "        as the conversation becomes more general.\n",
    "    </li>\n",
    "    <li>We are now seeing neural networks for doing this &mdash; quite similar to the ones for\n",
    "        Machine Translation: an encoder and a decoder, trained on, e.g., social media conversations.\n",
    "    </li>\n",
    "    <li>E.g. Google's Meena system:\n",
    "        <a href=\"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</a>\n",
    "        <a href=\"https://arxiv.org/pdf/2001.09977.pdf\">https://arxiv.org/pdf/2001.09977.pdf</a>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "<ul>\n",
    "    <li>There has been huge progress in NLP in the last couple of years.</li>\n",
    "    <li>In particular, using a variety of architectures, including Transformers, there has been a lot\n",
    "        of work on producing pretrained models that you can use as layers in your own architecture.\n",
    "    </li>\n",
    "    <li>One famous example is Google's BERT: <a href=\"https://github.com/google-research/bert\">https://github.com/google-research/bert</a>\n",
    "        <ul>\n",
    "            <li>Google claim that this has much improved their search engine's ability to answer\n",
    "                questions (as opposed to traditional keyword search): <a href=\"https://www.blog.google/products/search/search-language-understanding-bert/\">https://www.blog.google/products/search/search-language-understanding-bert/</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Another famous example is GPT-3, releaed in 2020. It has 175 billion parameters and produces text that is hard to distinguish from text produced by humans.\n",
    "        <ul>\n",
    "            <li>Consider this, for example: <a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\">https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>\n",
    "            </li>\n",
    "            <li>Or try this: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Of course, the big question, which we'll return to in the last lecture of the module is:\n",
    "        do these neural networks actually <em>understand</em> anything?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
